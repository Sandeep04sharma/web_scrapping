{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS edge\n",
    "options=EdgeOptions()\n",
    "options.use_chromium=True\n",
    "driver= Edge(options=options)\n",
    "\n",
    "# Chrome or firefox\n",
    "# from selenium import webdriver\n",
    "# driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url='https://www.ajio.com'\n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36 Edg/92.0.902.55'}\n",
    "driver.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=sqlite3.connect('GroomProducts.db')\n",
    "c=conn.cursor()\n",
    "# c.execute('''CREATE TABLE ProductList(Website TEXT, ProductLink TEXT,ProductName TEXT, \n",
    "#            ProductBrand TEXT, ProductCat TEXT, Sizes,Price INTEGER,MRP INTEGER, Gender TEXT,Description TEXT, PrimImgLink TEXT, SecImgLinks)''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Get Products links list of a given search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_links(search_term):\n",
    "    productsLink=[]\n",
    "    search_term=search_term.replace(' ','%20')\n",
    "    url='https://www.ajio.com/search/?text={}'.format(search_term)\n",
    "    driver.get(url)\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\") \n",
    "    i = 1\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        prod_List=soup.find_all('div','item rilrtl-products-list__item item')\n",
    "        for item in prod_List:\n",
    "            for link in item.find_all('a','rilrtl-products-list__link'):\n",
    "                if (base_url+link['href']) not in productsLink:\n",
    "                    productsLink.append(base_url+link['href'])\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "\n",
    "    return productsLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing in Sqlite3 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_prod_list=['men shirt' , 'men jeans' , 'men trouser', 'men underwear','men Vests' ,'men jacket','women sarees','women kaften', 'women top' , 'women leggings','women short', 'women dress', 'women jacket',\n",
    "#                   'women innerwear','men shoes','women footwear', 'men watches','women watches','men wallets','women handbags','men backpacks','women backpacks','men ties','pocket squares',\n",
    "#                   'men caps','women caps','men Mufflers','women Mufflers','women Gloves and masks','Mobile cases','women rings','women Jewellery','men socks','women socks']\n",
    "\n",
    "# search_prod_list=['men shoes','women caps','Mobile cases']  # already executed on trial bases, 4000 items stored successfully. \n",
    "search_prod_list =['men jacket']   #this is demo item list , u can uncomment the above list and can run program on this list ...\n",
    "                                              # but uncommenting that is not recommended, coz in beetween u lost the connection from server\n",
    "                                              # and this lead to generate error in program, so it is safer that u run program at a time only on 2-3 products list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a0887c1677f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_prod_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mproductsLinkList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_prod_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproductsLinkList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem\u001b[0m                                                                 \u001b[1;31m# category --- 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ac87c3e6f604>\u001b[0m in \u001b[0;36mget_prod_links\u001b[1;34m(search_term)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"window.scrollTo(0, {screen_height}*{i});\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscroll_pause_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0msoup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprod_List\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'item rilrtl-products-list__item item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in search_prod_list:\n",
    "    productsLinkList=get_prod_links(item)\n",
    "    print(item,len(productsLinkList))\n",
    "    category=item                                                                 # category --- 5\n",
    "\n",
    "    search_term=item.replace(' ','%20')\n",
    "       \n",
    "    for link in productsLinkList:\n",
    "        website=str('https://www.ajio.com/search/?text={}'.format(search_term))    #website....1\n",
    "        product_link =link                                                         # product link ..2\n",
    "        \n",
    "        driver.get(link)\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            name=soup.find('h1',class_='prod-name').text.strip()                    #Product name...3\n",
    "        except:\n",
    "            name=category\n",
    "        \n",
    "        try:\n",
    "            brand=soup.find('h2',class_='brand-name').text.strip()                   # brand...4\n",
    "\n",
    "        except:\n",
    "            brand='N.A.'\n",
    "        \n",
    "        try:\n",
    "            size=soup.find('div','size-variant-block')                               # size.........6\n",
    "            sizes = size.findChildren('div', {'size-swatch'}, recursive=True)\n",
    "            size_l =[]\n",
    "            for item in sizes:\n",
    "                size_l.append(item.text.strip())\n",
    "            size_list=json.dumps(size_l)                                              #..... 6...size array\n",
    "\n",
    "        except:\n",
    "            size_list='NA'\n",
    "    \n",
    "        try:\n",
    "            sprice=soup.find('div',class_='prod-sp').text.strip()                  # price   ...7\n",
    "            price=int(sprice.replace('Rs. ','').replace(',',''))\n",
    "\n",
    "        except:\n",
    "            price=0\n",
    "            \n",
    "        try:\n",
    "            mrp=soup.find('span',class_='prod-cp').text.strip()                   # MRP....8\n",
    "            MRP=int(mrp.replace('Rs. ','').replace(',',''))\n",
    "        except:\n",
    "            MRP=0\n",
    "\n",
    "        \n",
    "        if 'women' in item:                                                      # 9...gender\n",
    "            gender='Female'\n",
    "        elif 'men' in item:\n",
    "            gender='Male'\n",
    "    \n",
    "        elif item=='Mobile cases':\n",
    "            gender='For both'\n",
    "        else:\n",
    "            gender='Male'                                                       # 9...gender\n",
    "        \n",
    "\n",
    "        \n",
    "        try:\n",
    "            desc=soup.find('ul',class_='prod-list')                              # 10....desc\n",
    "            descr=[]\n",
    "            for item in desc:\n",
    "                descr.append(item.text.strip())\n",
    "\n",
    "            description=\", \".join(descr)                                          \n",
    "             \n",
    "        except:\n",
    "            description='Not available'                                         # 10...desc \n",
    "        \n",
    "        try:\n",
    "            images = soup.find('div', 'slick-track')\n",
    "            all_images = images.findChildren(\"img\" , recursive=True)\n",
    "\n",
    "            secondary_images_links = []\n",
    "            prim_image=all_images[0]['src']\n",
    "    \n",
    "            for  image in all_images[1:]:\n",
    "                secondary_images_links.append(image['src'])\n",
    "\n",
    "            sec_img=json.dumps(list(set(secondary_images_links)))\n",
    "\n",
    "        except:\n",
    "            prim_image='Not available'\n",
    "            sec_img='Not available'\n",
    "        \n",
    "#         result=(website,product_link,name,brand,category,size_list,price,MRP,gender,description,prim_image,sec_img)\n",
    "#         records.append(result)\n",
    "        print(f\"saving------{count}---\")\n",
    "        count+=1\n",
    "        c.execute('''INSERT INTO ProductList values(?,?,?,?,?,?,?,?,?,?,?,?)''',\n",
    "                 (website,product_link,name,brand,category,size_list,price,MRP,gender,description,prim_image,sec_img))\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()             #close the connection with database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqlite database viewer    https://inloop.github.io/sqlite-viewer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'count=1\\nproducts=[]\\nsearch_prod_list=[\\'men watches\\']\\n\\n\\n\\nfor item in search_prod_list:\\n    records=[]\\n    search_term=item.replace(\\' \\',\\'%20\\')\\n    category=item\\n    productsLinkList=get_prod_links(item)\\n    print(item,len(productsLinkList))\\n    \\n    for link in productsLinkList:\\n        \\n\\n        website=str(\\'https://www.ajio.com/search/?text={}\\'.format(search_term))             #website...1\\n\\n        product_link =link                                                                # product link ....2\\n        \\n        driver.get(link)\\n        soup=BeautifulSoup(driver.page_source,\\'html.parser\\')\\n\\n        try:\\n            name=soup.find(\\'h1\\',class_=\\'prod-name\\').text.strip()   #Product name...3\\n#         print(name)\\n        except:\\n            name=category\\n        try:\\n            brand=soup.find(\\'h2\\',class_=\\'brand-name\\').text.strip()    # brand...4\\n#         print(brand)\\n#         category=item   #   ....men clothing, women clothing, footwear..etc if no way to find, .item name hi likh denge....5\\n#         print(category)\\n        except:\\n            brand=\\'\\'\\n        try:\\n            size=soup.find(\\'div\\',\\'size-variant-block\\')                      # size............6\\n            sizes = size.findChildren(\\'div\\', {\\'size-swatch\\'}, recursive=True)\\n            size_l =[]\\n            for item in sizes:\\n                size_l.append(item.text.strip())\\n            size_list=json.dumps(size_l)                          #..... 6...size array\\n#         print(size_list)\\n        except:\\n            size_list=\\'All size\\'\\n    \\n        try:\\n            sprice=soup.find(\\'div\\',class_=\\'prod-sp\\').text.strip()  # 7...price=int(sprice).Rs. 209 andRs. 2,499 inko int bnana hai....7..\\n            price=int(sprice.replace(\\'Rs. \\',\\'\\').replace(\\',\\',\\'\\'))\\n#         print(price)\\n        except:\\n            price=\\'0\\'\\n        try:\\n            mrp=soup.find(\\'span\\',class_=\\'prod-cp\\').text.strip()    # MRP....8...integer bnana hai  Rs. 2,499\\n            MRP=int(mrp.replace(\\'Rs. \\',\\'\\').replace(\\',\\',\\'\\'))\\n        except:\\n            mrp=\\'0\\'                                                \\n            MRP=int(mrp.replace(\\'Rs. \\',\\'\\').replace(\\',\\',\\'\\'))\\n#         print(MRP)\\n        # 9...gender....search item name mai if item.find(\\'male\\'): gender male, elseif female, else both or in some cases again female\\n        \\n        gender=\\'male\\'\\n#         print(gender)\\n        \\n        try:\\n            desc=soup.find(\\'ul\\',class_=\\'prod-list\\')   # 10....desc\\n            descr=[]\\n            for item in desc:\\n                descr.append(item.text.strip())\\n\\n            description=\",\".join(descr)             # 10...desc ..string ...done\\n#         print(description)\\n#         prim_img=soup.find(\\'img\\',\\'rilrtl-lazy-img img-alignment zoom-cursor rilrtl-lazy-img-loaded\\').text\\n#         prim_image=prim_img[\\'src\\']              # 11 ...primary image...done\\n        \\n        except:\\n            description=\\'\\'\\n          # seconday image.....list hai, with help json.dumps karke array bna ke insert karna  ....12 ....\\n        try:\\n            images = soup.find(\\'div\\', \\'slick-track\\')\\n            all_images = images.findChildren(\"img\" , recursive=True)\\n\\n            secondary_images_links = []\\n            prim_image=all_images[0][\\'src\\']\\n    #         print(prim_image)\\n            for  image in all_images[1:]:\\n                secondary_images_links.append(image[\\'src\\'])\\n    #         for links in secondary_images_links:\\n    #             print(links) \\n            sec_img=json.dumps(secondary_images_links)\\n#         print(sec_img)\\n        except:\\n            prim_image=\\'\\'\\n            sec_img=\\'\\'\\n        \\n        result=(website,product_link,name,brand,category,size_list,price,MRP,gender,description,prim_image,sec_img)\\n        records.append(result)\\n\\n        print(f\"saving------{count}---\")\\n        count+=1\\n        with open(\\'results.csv\\',\\'w\\',newline=\\'\\',encoding=\\'utf8\\') as f:\\n                writer=csv.writer(f)\\n                writer.writerow([\\'Website\\',\\'Prod_Url\\',\\'Name\\',\\'Brand\\',\\'Prod_cat\\',\\'Sizes\\',\\'Price\\',\\'MRP\\',\\'Gender\\',\\'Description\\',\\'PrimaryImg\\',\\'Sec_imag\\'])\\n                writer.writerows(records)        \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''count=1\n",
    "products=[]\n",
    "search_prod_list=['men watches']\n",
    "\n",
    "\n",
    "\n",
    "for item in search_prod_list:\n",
    "    records=[]\n",
    "    search_term=item.replace(' ','%20')\n",
    "    category=item\n",
    "    productsLinkList=get_prod_links(item)\n",
    "    print(item,len(productsLinkList))\n",
    "    \n",
    "    for link in productsLinkList:\n",
    "        \n",
    "\n",
    "        website=str('https://www.ajio.com/search/?text={}'.format(search_term))             #website...1\n",
    "\n",
    "        product_link =link                                                                # product link ....2\n",
    "        \n",
    "        driver.get(link)\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            name=soup.find('h1',class_='prod-name').text.strip()   #Product name...3\n",
    "#         print(name)\n",
    "        except:\n",
    "            name=category\n",
    "        try:\n",
    "            brand=soup.find('h2',class_='brand-name').text.strip()    # brand...4\n",
    "#         print(brand)\n",
    "#         category=item   #   ....men clothing, women clothing, footwear..etc if no way to find, .item name hi likh denge....5\n",
    "#         print(category)\n",
    "        except:\n",
    "            brand=''\n",
    "        try:\n",
    "            size=soup.find('div','size-variant-block')                      # size............6\n",
    "            sizes = size.findChildren('div', {'size-swatch'}, recursive=True)\n",
    "            size_l =[]\n",
    "            for item in sizes:\n",
    "                size_l.append(item.text.strip())\n",
    "            size_list=json.dumps(size_l)                          #..... 6...size array\n",
    "#         print(size_list)\n",
    "        except:\n",
    "            size_list='All size'\n",
    "    \n",
    "        try:\n",
    "            sprice=soup.find('div',class_='prod-sp').text.strip()  # 7...price=int(sprice).Rs. 209 andRs. 2,499 inko int bnana hai....7..\n",
    "            price=int(sprice.replace('Rs. ','').replace(',',''))\n",
    "#         print(price)\n",
    "        except:\n",
    "            price='0'\n",
    "        try:\n",
    "            mrp=soup.find('span',class_='prod-cp').text.strip()    # MRP....8...integer bnana hai  Rs. 2,499\n",
    "            MRP=int(mrp.replace('Rs. ','').replace(',',''))\n",
    "        except:\n",
    "            mrp='0'                                                \n",
    "            MRP=int(mrp.replace('Rs. ','').replace(',',''))\n",
    "#         print(MRP)\n",
    "        # 9...gender....search item name mai if item.find('male'): gender male, elseif female, else both or in some cases again female\n",
    "        \n",
    "        gender='male'\n",
    "#         print(gender)\n",
    "        \n",
    "        try:\n",
    "            desc=soup.find('ul',class_='prod-list')   # 10....desc\n",
    "            descr=[]\n",
    "            for item in desc:\n",
    "                descr.append(item.text.strip())\n",
    "\n",
    "            description=\",\".join(descr)             # 10...desc ..string ...done\n",
    "#         print(description)\n",
    "#         prim_img=soup.find('img','rilrtl-lazy-img img-alignment zoom-cursor rilrtl-lazy-img-loaded').text\n",
    "#         prim_image=prim_img['src']              # 11 ...primary image...done\n",
    "        \n",
    "        except:\n",
    "            description=''\n",
    "          # seconday image.....list hai, with help json.dumps karke array bna ke insert karna  ....12 ....\n",
    "        try:\n",
    "            images = soup.find('div', 'slick-track')\n",
    "            all_images = images.findChildren(\"img\" , recursive=True)\n",
    "\n",
    "            secondary_images_links = []\n",
    "            prim_image=all_images[0]['src']\n",
    "    #         print(prim_image)\n",
    "            for  image in all_images[1:]:\n",
    "                secondary_images_links.append(image['src'])\n",
    "    #         for links in secondary_images_links:\n",
    "    #             print(links) \n",
    "            sec_img=json.dumps(secondary_images_links)\n",
    "#         print(sec_img)\n",
    "        except:\n",
    "            prim_image=''\n",
    "            sec_img=''\n",
    "        \n",
    "        result=(website,product_link,name,brand,category,size_list,price,MRP,gender,description,prim_image,sec_img)\n",
    "        records.append(result)\n",
    "\n",
    "        print(f\"saving------{count}---\")\n",
    "        count+=1\n",
    "        with open('results.csv','w',newline='',encoding='utf8') as f:\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerow(['Website','Prod_Url','Name','Brand','Prod_cat','Sizes','Price','MRP','Gender','Description','PrimaryImg','Sec_imag'])\n",
    "                writer.writerows(records)        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading data from database using pandas or u can drag and drop database here:- https://inloop.github.io/sqlite-viewer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_sql_query(\"SELECT * FROM TryProductList \",conn)\n",
    "# print(df.head(20))\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
